{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60928d82-77db-4554-af68-6e2c42c14708",
   "metadata": {},
   "source": [
    "# Project 2 Part 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb3d71d-f642-4f65-ae35-6fa8e0aeb143",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "**Part 1**\n",
    "- Define a filepaths dictionary and save it to config/filepaths.json  to include file paths for each component you will save (review below).\n",
    "- Copy your best models from part 6 into the new notebook.\n",
    "    - Note: update your code to define the final public-facing class labels.\n",
    "- Saving the data\n",
    "    - For the Machine Learning Model\n",
    "        - Save training data: X_train, y_train\n",
    "        - Save testing data: X_test, y_test\n",
    "        - Save target_lookup dictionary and/or label encoder\n",
    "        - Save best model\n",
    "    - For Deep NLP Model\n",
    "        - Save training data: X_train, y_train\n",
    "        - Save testing data: X_test, y_test\n",
    "        - Save best neural network\n",
    "            - Note: use safe_format='tf' to save model in a folder of repo-friendly files\n",
    " \n",
    "**Part 2**\n",
    "- Create a streamlit app for getting predictions for a user-entered text from your loaded model\n",
    "- (Optional but recommended); Include a Lime Text Explainer explanation for the prediction.\n",
    "- Include the ability to load the training and test data to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b7494-ebee-477b-9a3b-7dc0d3a181db",
   "metadata": {},
   "source": [
    "### Imports and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1121c26a-4425-4684-9c68-087b4abb16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pprint import pprint\n",
    "import os, json, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import joblib\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Random seed\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125066ab-e36f-4bb2-a118-be8a820c6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import custom_functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa36b4-194f-47d5-a0b1-e54ece919a13",
   "metadata": {},
   "source": [
    "### Creating Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85948ba-3cbb-4bd5-b06d-c68b9e1c0511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'ml': {'encoder': 'Data-NLP/encoder.joblib',\n",
      "                 'target_lookup': 'Data-NLP/target_lookup.joblib',\n",
      "                 'test': 'Data-NLP/testing-data.joblib',\n",
      "                 'train': 'Data-NLP/training-data.joblib'},\n",
      "          'source': {'processed': 'Data-NLP/reviews_processed.joblib',\n",
      "                     'raw': 'Data-NLP/movie_reviews_v2.csv'},\n",
      "          'tf': {'test': 'Data-NLP/test_ds.joblib',\n",
      "                 'train': 'Data-NLP/train_ds.joblib'}},\n",
      " 'images': {'wordcloud': 'images/movie_reviews_wordcloud.png'},\n",
      " 'models': {'GRU': 'models/GRU/',\n",
      "            'random_forest': 'models/random_forest/rf_reg.joblib'}}\n"
     ]
    }
   ],
   "source": [
    "# Define a filepath dictionary\n",
    "FPATHS = dict(\n",
    "    \n",
    "    # Data\n",
    "    data={\n",
    "        # Source data files\n",
    "        \"source\": {\n",
    "            \"raw\": 'Data-NLP/movie_reviews_v2.csv',\n",
    "            \"processed\": \"Data-NLP/reviews_processed.joblib\",\n",
    "        },\n",
    "        # Machine Learning data files\n",
    "        \"ml\": {\n",
    "            \"train\": \"Data-NLP/training-data.joblib\",\n",
    "            \"test\": \"Data-NLP/testing-data.joblib\",\n",
    "            \"target_lookup\": \"Data-NLP/target_lookup.joblib\",\n",
    "            \"encoder\": \"Data-NLP/encoder.joblib\",\n",
    "        },\n",
    "        # Tensorflow data files\n",
    "        \"tf\": {\n",
    "            \"train\": \"Data-NLP/train_ds.joblib\",\n",
    "            \"test\": \"Data-NLP/test_ds.joblib\",\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    # Models\n",
    "    models={\n",
    "        \"random_forest\": \"models/random_forest/rf_reg.joblib\",\n",
    "        \"GRU\": \"models/GRU/\", # We haven't saved this yet\n",
    "    },\n",
    "    \n",
    "    # Images/EDA\n",
    "    images={\n",
    "        \"wordcloud\": \"images/movie_reviews_wordcloud.png\",\n",
    "    },\n",
    ")\n",
    "pprint(FPATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89df84a5-40a8-42cb-8727-91d8eeb150c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filepaths in a config folder\n",
    "os.makedirs('config/', exist_ok = True)\n",
    "FPATHS_FILE = 'config/filepaths.json'\n",
    "with open(FPATHS_FILE, 'w') as f:\n",
    "    json.dump(FPATHS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8236552-f803-4934-a2e1-7bf54f3a2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from \"Creating a File Structure\"\n",
    "fn.create_directories_from_paths(FPATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c879b72c-9d21-42ff-abee-94db4f9f1544",
   "metadata": {},
   "source": [
    "### Preprocessing and Saving Data - Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45ffb53-5783-4d9e-94f5-11cd8634c739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_title</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>highlow_rating</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokenized_joined</th>\n",
       "      <th>lemmatized_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57086ff5c3a3681d29001512</td>\n",
       "      <td>7443</td>\n",
       "      <td>tt0120630</td>\n",
       "      <td>Chicken Run</td>\n",
       "      <td>A guilty pleasure for me personally, as I love...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>high</td>\n",
       "      <td>[guilty, pleasure, personally, love, great, es...</td>\n",
       "      <td>[guilty, pleasure, personally, love, great, es...</td>\n",
       "      <td>guilty pleasure personally love great escape w...</td>\n",
       "      <td>guilty pleasure personally love great escape w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  review_id  movie_id    imdb_id original_title  \\\n",
       "1  57086ff5c3a3681d29001512      7443  tt0120630    Chicken Run   \n",
       "\n",
       "                                              review  rating highlow_rating  \\\n",
       "1  A guilty pleasure for me personally, as I love...     9.0           high   \n",
       "\n",
       "                                           tokenized  \\\n",
       "1  [guilty, pleasure, personally, love, great, es...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "1  [guilty, pleasure, personally, love, great, es...   \n",
       "\n",
       "                                    tokenized_joined  \\\n",
       "1  guilty pleasure personally love great escape w...   \n",
       "\n",
       "                                   lemmatized_joined  \n",
       "1  guilty pleasure personally love great escape w...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load processed review dataframe \n",
    "df = joblib.load(\"Data-NLP/reviews_processed.joblib\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539bfb0-6fab-4ec1-9103-090bad448d15",
   "metadata": {},
   "source": [
    "**Apply New Labels and TTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7bb00f-3f40-4a0b-a594-60620796ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply new labels for public-facing app\n",
    "\n",
    "# Rename columns\n",
    "df = df.rename(columns={'review': 'Review', 'highlow_rating': 'Rating'})\n",
    "# Format ratings labels\n",
    "rating_dict = {'low': 'Low Rating', 'high': 'High Rating'}\n",
    "\n",
    "# Use original review column as X and classification target column as y\n",
    "y = df['Rating'].replace(rating_dict)\n",
    "X = df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab3ba22-9915-4c66-abb1-782dcc039b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Low Rating     0.510474\n",
       "High Rating    0.489526\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train Test Split\n",
    "X_train, X_test, y_train_str, y_test_str = train_test_split(X, y, random_state=42)\n",
    "y_train_str.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ef5c0-dec2-4847-9df3-4bbcfa4e0698",
   "metadata": {},
   "source": [
    "**Obtain Encoder Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5aca80-ee6e-4c82-95dc-e9d2f90556e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting unique classes to convert \n",
    "class_names = y_train_str.unique()\n",
    "\n",
    "# Fit the label encoder on unique class names\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbc3a85-9854-4e08-bb2f-97d0fd10e654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the y_train_str and y_test_str with encoder\n",
    "y_train = encoder.transform(y_train_str)\n",
    "y_test = encoder.transform(y_test_str)\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9444d3-ff72-4366-8763-fb698a4732af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'High Rating', 1: 'Low Rating'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract class names from the encoder\n",
    "classes = encoder.classes_\n",
    "\n",
    "# Get the encoded values for each both classes\n",
    "class_codes = encoder.transform(classes)\n",
    "\n",
    "# Making lookup dictionary to find the encoded label's original name\n",
    "target_lookup = dict(zip(class_codes, classes))\n",
    "# Verify dictionary\n",
    "target_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ac757-6dc4-4c4b-8463-8b4133310b1e",
   "metadata": {},
   "source": [
    "**Creating the Model from Part 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "822a395b-94e6-4fd9-98b6-3ef9968641d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build text vectorizer and classification model\n",
    "# Create a sklearn text vectorizer. Consider stopwords, punc, etc.\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "# Create classification model\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b84ae15-2110-4d6f-960f-aa9ad9e84d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;classifier&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;classifier&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer(stop_words='english')),\n",
       "                ('classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an sklearn modeling pipeline\n",
    "rf_pipe = Pipeline([\n",
    "    ('vectorizer', tfidf_vectorizer), \n",
    "    ('classifier', rf_clf),\n",
    "])\n",
    "\n",
    "# Fit and evaluate the model\n",
    "rf_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b9c19-fcbe-44e6-8fbd-0853dfef6922",
   "metadata": {},
   "source": [
    "**Saving the Data and Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8665a778-b86f-4cde-9f74-fdb1f5c5d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the data\n",
    "# Save the training data\n",
    "train_path = FPATHS['data']['ml']['train']\n",
    "joblib.dump([X_train, y_train], train_path)\n",
    "\n",
    "# Save the testing data\n",
    "test_path = FPATHS['data']['ml']['test']\n",
    "joblib.dump([X_test, y_test], test_path)\n",
    "\n",
    "# Save the label encoder\n",
    "encoder_path = FPATHS['data']['ml']['encoder']\n",
    "joblib.dump(encoder, encoder_path);\n",
    "\n",
    "# Save the target_lookup\n",
    "lookup_path = FPATHS['data']['ml']['target_lookup']\n",
    "joblib.dump(target_lookup, lookup_path);\n",
    "\n",
    "# Save the model\n",
    "model_path = FPATHS['models']['random_forest']\n",
    "joblib.dump(rf_pipe, model_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d713c-ddb1-43d8-ab97-16735ed296c0",
   "metadata": {},
   "source": [
    "### Preprocessing and Saving Data - Deep NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d7da084-3792-469c-a460-b87e4e25d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data using dictionary\n",
    "[X_train, y_train] = joblib.load(FPATHS['data']['ml']['train'])\n",
    "[X_test, y_test]  = joblib.load(FPATHS['data']['ml']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539faebf-7b74-4c1e-ac8e-dd38120b7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataset object from train data\n",
    "train_tf = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# Make a dataset object from test data\n",
    "test_tf = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b77683f-bd93-422a-bf70-90f8dae3407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "train_tf = train_tf.shuffle(buffer_size=len(train_tf),reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cfa9c68-9c77-4437-8d16-bd9a0b37e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - train:\t1360 samples \t(43 batches)\n",
      "    - val:  \t453 samples \t(15 batches)\n"
     ]
    }
   ],
   "source": [
    "# Set the ratio of the train, validation, test split\n",
    "split_train = .75\n",
    "split_val =  .25\n",
    "# Calculate the number of samples for training and validation data \n",
    "n_train_samples =  int(len(train_tf) * split_train)\n",
    "n_val_samples = int(len(train_tf) * split_val)\n",
    "# Set the batch size\n",
    "BATCH_SIZE =32\n",
    "import math\n",
    "# math.ceil will round up\n",
    "# How many batches? \n",
    "n_train_batches = math.ceil(n_train_samples/BATCH_SIZE)\n",
    "n_val_batches = math.ceil(n_val_samples/BATCH_SIZE)\n",
    "print(f\"    - train:\\t{n_train_samples} samples \\t({n_train_batches} batches)\")\n",
    "print(f\"    - val:  \\t{n_val_samples} samples \\t({n_val_batches} batches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d778f4-7c2f-4636-b295-6a77373a6776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are 43 training batches.\n",
      " There are 15 validation batches.\n"
     ]
    }
   ],
   "source": [
    "# Use take and skip to define each set\n",
    "train_ds = train_tf.take(n_train_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Skip over the training batches and take the validation batches\n",
    "val_ds = train_tf.skip(n_train_samples).take(n_val_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Confirm the number of batches in each\n",
    "print (f' There are {len(train_ds)} training batches.')\n",
    "print (f' There are {len(val_ds)} validation batches.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d362e8c8-a84e-4b16-9f8c-006c91bad0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are 19 testing batches.\n"
     ]
    }
   ],
   "source": [
    "# Put the test data into batches also\n",
    "test_ds = test_tf.batch(batch_size = BATCH_SIZE)\n",
    "# How many batches\n",
    "print (f' There are {len(test_ds)} testing batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd082e-eb93-4081-8c7c-893c2ad4982e",
   "metadata": {},
   "source": [
    "**Saving the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07b127f6-ca08-4d16-be4e-2b979bc239e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training dataset object\n",
    "train_ds_fpath = FPATHS['data']['tf']['train']\n",
    "tf.data.Dataset.save(train_ds, train_ds_fpath)\n",
    "# Save testing dataset object\n",
    "test_ds_fpath = FPATHS['data']['tf']['test']\n",
    "tf.data.Dataset.save(test_ds, test_ds_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ef2baa0-137b-471b-9268-130a5419b5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['High Rating', 'Low Rating'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define classes\n",
    "encoder = joblib.load(FPATHS['data']['ml']['encoder'])\n",
    "classes = encoder.classes_\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43fb80a3-ee81-4337-84b0-debf662eb88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keras text vect layer for RNN sequence model\n",
    "SEQUENCE_LENGTH = 100\n",
    "sequence_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32556df9-8eb4-4047-a1d0-17043c0a050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the text from ds_train\n",
    "ds_texts = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Train model\n",
    "sequence_vectorizer.adapt(ds_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c352f106-4b3d-4dcd-aebf-0dd44ec3784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of vocab\n",
    "vocab = sequence_vectorizer.get_vocabulary()\n",
    "int_to_str = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Programmatically define size of vocab from vectorization layer\n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()\n",
    "\n",
    "# Define classes variable\n",
    "classes = y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb2d0bf7-377b-456f-aef0-eb431a895f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an RNN with textVector layer\n",
    "def build_bidir_GRU(text_vectorization_layer):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE, \n",
    "                         output_dim=250, \n",
    "                         input_length=SEQUENCE_LENGTH)])\n",
    "\n",
    "    # Two bidirectional GrU layers    \n",
    "    model.add(layers.Bidirectional(layers.GRU(128, return_sequences=True)))\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Bidirectional(layers.GRU(64, return_sequences=True)))\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(classes), activation='sigmoid'))\n",
    "    \n",
    "    # Compile\n",
    "    optimizer = optimizers.legacy.Adam()\n",
    "    model.compile(optimizer=optimizer,  \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "881c0632-bbb8-4bf9-92af-83f8580ba9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include callbacks\n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3c538ec-7766-4d1c-924a-32d08d4330eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 100)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 250)          5736000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 256)         291840    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 100, 128)         123648    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100, 128)          0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,151,746\n",
      "Trainable params: 6,151,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "43/43 [==============================] - 15s 225ms/step - loss: 0.6839 - accuracy: 0.5456 - val_loss: 0.6671 - val_accuracy: 0.5806\n",
      "Epoch 2/30\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 0.4331 - accuracy: 0.7941 - val_loss: 0.5860 - val_accuracy: 0.6203\n",
      "Epoch 3/30\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 0.0935 - accuracy: 0.9669 - val_loss: 0.7267 - val_accuracy: 0.6623\n",
      "Epoch 4/30\n",
      "43/43 [==============================] - 8s 195ms/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 0.5920 - val_accuracy: 0.7616\n",
      "Epoch 5/30\n",
      "43/43 [==============================] - 8s 193ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.6656 - val_accuracy: 0.7616\n",
      "Epoch 6/30\n",
      "43/43 [==============================] - 8s 195ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.7596 - val_accuracy: 0.7395\n",
      "Epoch 7/30\n",
      "43/43 [==============================] - 8s 195ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7499 - val_accuracy: 0.7395\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = build_bidir_GRU(sequence_vectorizer)\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "492f1da6-2694-4327-a249-6dcd5b8ad126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GRU/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/GRU/assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model_fpath = FPATHS['models']['GRU']\n",
    "# tf.keras.models.save_model(model, model_fpath, save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
